#________________________________________________________________________________________________________DST_BEAM__
#
BEAM_DST_STREAMING_EVENT_run2pp:
 
   # At the moment, an extra _ in here will throw off cups b/c it parses out 
   # run, build, dsttype, etc... from the output filename... so DST_BEAM_run1aua
   # will need to wait...
   params:
     name:       DST_STREAMING_EVENT_run2pp
     build:      new
     build_name: new
     dbtag:      2024p001
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run_cosmics.sh
     payload :   ./slurp-examples/sPHENIX/cosmics/
     mem     :   4096MB
     neventsper: 10000
     comment:    "---"


   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   # For now, require GL1 files...
   #--           (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
   #              sum( case when filename like '/bbox%/beam%mvtx%' then 1 else 0 end )>0 
   #
   # sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )>0 and
   #
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/TPC%beam%.evt'   and lastevent>2 ) or
             (filename  like '/bbox%/TPOT%beam%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/beam_intt%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/GL1_beam%'       and lastevent>2 ) or
             (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
           )
           {run_condition}

         group by runnumber
         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )>0 and
                (
                   sum( case when filename like '/bbox%/TPC%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/TPOT%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam%intt%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam_mvtx%.evt' then 1 else 0 end )>0 
                )
         order by runnumber
                {limit_condition}
              ;

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/tpcbeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/testlogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run_cosmics.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
     transfer_output_files : ''
#    transfer_output_files : 'stdout.log,stderr.log'
     priority : '3800'



__DST_BEAM_nogl1__:
   params:
     name:       DST_BEAM_run2pp
     build:      new
     build_name: new
     dbtag:      2023p013
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run_cosmics.sh
     payload :   ./slurp-examples/sPHENIX/cosmics/
     mem     :   8192MB
     neventsper: 5000
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/TPC%beam%.evt'   and lastevent>2 ) or
             (filename  like '/bbox%/TPOT%beam%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/beam_intt%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/GL1_beam%'       and lastevent>2 ) or 
             (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
           )
                {run_condition}

         group by runnumber
         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )<=0 and
                (
                   sum( case when filename like '/bbox%/TPC%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/TPOT%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam%intt%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam_mvtx%.evt' then 1 else 0 end )>0 
                )
         order by runnumber
                {limit_condition};
      
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/tpcbeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/testlogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"
   job:
     executable            : "{payload}/run_cosmics.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
#    transfer_output_files : '{logbase}.out,{logbase}.err'
     priority : '3800'



#_______________________________________________________________________________________________________DST_EVENT__
#
# kaedama.py --rule DST_EVENT --config examples/sPHENIX/DST_EVENT_aua23.yaml --runs ...
#
BEAM_DST_TPCCALIB_run2pp:
 
   # DST_EVENT works from a pre-built set of run lists.
   params:
     name:       DST_TPCCALIB_run2pp
     build:      new
     build_name: new
     dbtag:      2024p001
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run_cosmics.sh
     payload :   ./slurp-examples/sPHENIX/cosmics/
     neventsper: 100
#    mem     :   2048MB
     mem     :   8192MB

   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/tpc/calib/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
                filename     like '/bbox%/TPC%calib%.evt'
                {run_condition}
         group by runnumber
         having
                every(transferred_to_sdcc)                                            
         order by runnumber
                {limit_condition}
              ;              

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/tpccalib/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/testlogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run_cosmics.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
     priority : '3800'





DST_BEAM_INTT_RAW:
 
   # At the moment, an extra _ in here will throw off cups b/c it parses out 
   # run, build, dsttype, etc... from the output filename... so DST_BEAM_run1aua
   # will need to wait...
   params:
     name:       DST_INTT_RAW_beam
     build:      new
     build_name: new
     dbtag:      2023p011
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run_cosmics.sh
     payload :   ./slurp-examples/sPHENIX/cosmics/
     mem     :   4096MB
     neventsper: 500000

   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   # For now, require GL1 files...
   #             (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
   #               sum( case when filename like '/bbox%/beam%mvtx%' then 1 else 0 end )>0 
   #
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/TPOT%beam%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/beam_intt%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/GL1_beam%'       and lastevent>2 ) or
             (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
           )
                {run_condition}
           and runnumber>=35590

         group by runnumber
         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )>0 and
                (
                   sum( case when filename like '/bbox%/TPOT%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam%intt%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam_mvtx%.evt' then 1 else 0 end )>0 
                )
         order by runnumber
                {limit_condition}
              ;              

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/inttbeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/testlogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run_cosmics.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
#    transfer_output_files : '{logbase}.out,{logbase}.err'
#    transfer_output_files : 'stdout.log,stderr.log'
     priority : '3800'



DST_BEAM_NO_INTT:
 
   # At the moment, an extra _ in here will throw off cups b/c it parses out 
   # run, build, dsttype, etc... from the output filename... so DST_BEAM_run1aua
   # will need to wait...
   params:
     name:       DST_BEAM_run2pp
     build:      new
     build_name: new
     dbtag:      2023p013
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run_cosmics.sh
     payload :   ./slurp-examples/sPHENIX/cosmics/
     mem     :   2048MB
     neventsper: 5000

   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   # For now, require GL1 files...
   #              (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
   #                   sum( case when filename like '/bbox%/beam%mvtx%' then 1 else 0 end )>0 
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/TPC%beam%.evt'   and lastevent>2 ) or
             (filename  like '/bbox%/TPOT%beam%.evt'  and lastevent>2 ) or
             (filename  like '/bbox%/GL1_beam%'       and lastevent>2 ) or
             (filename  like '/bbox%/beam_mvtx%.evt'  and lastevent>2 )
           )
                {run_condition}

         group by runnumber
         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )>0 and
                (
                   sum( case when filename like '/bbox%/TPC%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/TPOT%beam%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/beam_mvtx%.evt' then 1 else 0 end )>0 
                )
         order by runnumber
                {limit_condition}
              ;              

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/tpcbeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/testlogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run_cosmics.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
#    transfer_output_files : '{logbase}.out,{logbase}.err'
#    transfer_output_files : 'stdout.log,stderr.log'
     priority : '3800'
#_______________________________________________________________________________________________________DST_EVENT__



#______________________________________________________________________________________________DST_BEAM_TRIGGERED__
#
BEAM_DST_TRIGGERED_EVENT_run2pp:
 
   # At the moment, an extra _ in here will throw off cups b/c it parses out 
   # run, build, dsttype, etc... from the output filename... so DST_BEAM_run1aua
   # will need to wait...
   params:
     name:       DST_TRIGGERED_EVENT_run2pp
     build:      new
     build_name: new
     dbtag:      2024p001
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run.sh
     payload :   ./slurp-examples/sPHENIX/triggers/
     mem     :   2048MB
     neventsper: 10000

   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   # For now, require GL1 files...
   #
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/%emcal%beam%'    and lastevent>2 ) or 
             (filename  like '/bbox%/%HCal%beam%'     and lastevent>2 ) or
             (filename  like '/bbox%/GL1_beam%'       and lastevent>2 ) or
             (filename  like '/bbox%/%mbd%beam%'      and lastevent>2 ) or
             (filename  like '/bbox%/%ZDC%beam%'      and lastevent>2 )
           )
           {run_condition}

         group by runnumber

         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )>0 and
                (
                   sum( case when filename like '/bbox%/%emcal%beam%'  then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/%HCal%beam%'   then 1 else 0 end )>0 
                )

         order by runnumber
                {limit_condition}

              ;              

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/calobeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/calologs/run_$(rungroup)"
     condor :        "/tmp/calologs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
#    transfer_output_files : '{logbase}.out,{logbase}.err'
     priority : '3800'


#______________________________________________________________________________________________DST_BEAM_TRIGGERED__
#
__DST_BEAM_TRIGGERED_nogl1__:
 
   # At the moment, an extra _ in here will throw off cups b/c it parses out 
   # run, build, dsttype, etc... from the output filename... so DST_BEAM_run1aua
   # will need to wait...
   params:
     name:       DST_TRIGGERED_RAW_beam
     build:      new
     build_name: new
     dbtag:      2023p015
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run.sh
     payload :   ./slurp-examples/sPHENIX/triggers/
     mem     :   2048MB
     neventsper: 10000

   #
   # input query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #
   # For now, require GL1 files...
   #
   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/beam/
      query: |-
         select 
                'daqdb/filelist'                                                                                    as source      , 
                runnumber                                                                                                          , 
                0                                                                                                   as segment     , 
                string_agg( distinct split_part(filename,'/',-1), ' ' )                                             as files       ,   
                string_agg( distinct split_part(filename,'/',-1) || ':' || firstevent || ':' || lastevent, ' ' )    as fileranges  
         from  
                filelist
         where 
           ( 
             (filename  like '/bbox%/%emcal%beam%'    and lastevent>2 ) or 
             (filename  like '/bbox%/%HCal%beam%'     and lastevent>2 ) or
             (filename  like '/bbox%/%mbd%beam%'      and lastevent>2 ) or
             (filename  like '/bbox%/%ZDC%beam%'      and lastevent>2 )
           )
           {run_condition}

         group by runnumber

         having
                every(transferred_to_sdcc)   and
                max(lastevent)>1000          and
                sum( case when filename like '/bbox%/GL1_beam%' then 1 else 0 end )<=0 and
                (
                   sum( case when filename like '/bbox%/%emcal%beam%'  then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/%HCal%beam%'   then 1 else 0 end )>0 
                )

         order by runnumber
                {limit_condition}

              ;              

   # TODO:  Need to add error checking to make sure that outdir, logdir, etc... are quoted properly.  Else, this will cause problems with argument substitution
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/calobeam/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/calologs/run_$(rungroup)"
     condor :        "/tmp/calologs/run_$(rungroup)"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {neventsper} {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
#    transfer_output_files : '{logbase}.out,{logbase}.err'
     priority : '3800'


DST_TRKR_HIT_SET_beam:
   # DST_EVENT works from a pre-built set of run lists.
   params:
     name:       DST_TRKR_HIT_run2pp
     build:      new
     build_name: new
     dbtag:      2024p001
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run.sh
     payload :   ./slurp-examples/sPHENIX/TrackingProduction/
     mem     :   2048MB

   input:
      db: fc
      query: |-
         select 
                'filecatalog/datasets'   as source       ,
                runnumber                                ,
                segment                                  ,
                filename                    as files     ,
                filename || ':0:' || events as fileranges
         from  
                datasets
         where
                filename like 'DST_BEAM_run2pp_new%'
                {run_condition}
         order by runnumber
                {limit_condition}
              ;              
   filesystem:  
     outdir : "/sphenix/lustre01/sphnxpro/physics/slurp/tracking/run_$(rungroup)"
     logdir : "file:///sphenix/data/data02/sphnxpro/trackinglogs/run_$(rungroup)"
     condor :        "/tmp/testlogs/run_$(rungroup)"

   job:
     executable            : "{payload}/run.sh"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) {outdir} $(build) $(tag) $(inputs) $(ranges) {logdir}"
     output_destination    : '{logdir}'
     transfer_input_files  : "{payload},cups.py,odbc.ini"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"
     transfer_output_files : ""
     priority : '3800'
